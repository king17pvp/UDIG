{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eec0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np, pickle, random\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c428c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from dig import DiscretetizedIntegratedGradients\n",
    "from attributions import run_dig_explanation\n",
    "from metrics import eval_log_odds, eval_comprehensiveness, eval_sufficiency\n",
    "import monotonic_paths\n",
    "from captum.attr._utils.common import _format_input_baseline, _reshape_and_sum, _validate_input, _format_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b49f587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a069240c50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94a3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model= AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8181a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2dcc539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inputs_embeds, attention_mask=None):\n",
    "    return model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)[0]\n",
    "\n",
    "def nn_forward_func(input_embed, attention_mask=None, position_embed=None, type_embed=None, return_all_logits=False):\n",
    "    global model\n",
    "    embeds\t= input_embed + position_embed\n",
    "    embeds\t= model.distilbert.embeddings.dropout(model.distilbert.embeddings.LayerNorm(embeds))\n",
    "    pred\t= predict(model, embeds, attention_mask=attention_mask)\n",
    "    if return_all_logits:\n",
    "        return pred\n",
    "    else:\n",
    "        return pred.max(1).values\n",
    "\n",
    "def load_mappings(dataset, knn_nbrs=500):\n",
    "    with open(f'knn/distilbert_{dataset}_{knn_nbrs}.pkl', 'rb') as f:\n",
    "        [word_idx_map, word_features, adj] = pickle.load(f)\n",
    "    word_idx_map\t= dict(word_idx_map)\n",
    "\n",
    "    return word_idx_map, word_features, adj\n",
    "\n",
    "def construct_input_ref_pair(tokenizer, text, ref_token_id, sep_token_id, cls_token_id, device):\n",
    "\ttext_ids\t\t= tokenizer.encode(text, add_special_tokens=False, truncation=True,max_length=tokenizer.max_len_single_sentence)\n",
    "\tinput_ids\t\t= [cls_token_id] + text_ids + [sep_token_id]\t# construct input token ids\n",
    "\tref_input_ids\t= [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\t# construct reference token ids\n",
    "\n",
    "\treturn torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids, device):\n",
    "\tseq_length\t\t\t= input_ids.size(1)\n",
    "\tposition_ids\t\t= torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "\tref_position_ids\t= torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "\tposition_ids\t\t= position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\tref_position_ids\t= ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "\treturn position_ids, ref_position_ids\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, device):\n",
    "\tseq_len\t\t\t\t= input_ids.size(1)\n",
    "\ttoken_type_ids\t\t= torch.tensor([[0] * seq_len], dtype=torch.long, device=device)\n",
    "\tref_token_type_ids\t= torch.zeros_like(token_type_ids, dtype=torch.long, device=device)\n",
    "\treturn token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "\treturn torch.ones_like(input_ids)\n",
    "\n",
    "def get_word_embeddings():\n",
    "\tglobal model\n",
    "\treturn model.distilbert.embeddings.word_embeddings.weight\n",
    "\n",
    "def construct_word_embedding(model, input_ids):\n",
    "\treturn model.distilbert.embeddings.word_embeddings(input_ids)\n",
    "\n",
    "def construct_position_embedding(model, position_ids):\n",
    "\treturn model.distilbert.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "def construct_type_embedding(model, type_ids):\n",
    "\treturn model.distilbert.embeddings.token_type_embeddings(type_ids)\n",
    "\n",
    "def construct_sub_embedding(model, input_ids, ref_input_ids, position_ids, ref_position_ids):\n",
    "\tinput_embeddings\t\t\t\t= construct_word_embedding(model, input_ids)\n",
    "\tref_input_embeddings\t\t\t= construct_word_embedding(model, ref_input_ids)\n",
    "\tinput_position_embeddings\t\t= construct_position_embedding(model, position_ids)\n",
    "\tref_input_position_embeddings\t= construct_position_embedding(model, ref_position_ids)\n",
    "# \tinput_type_embeddings\t\t\t= construct_type_embedding(model, type_ids)\n",
    "# \tref_input_type_embeddings\t\t= construct_type_embedding(model, ref_type_ids)\n",
    "\n",
    "\treturn \t(input_embeddings, ref_input_embeddings), \\\n",
    "\t\t\t(input_position_embeddings, ref_input_position_embeddings)\n",
    "\n",
    "def get_base_token_emb(device):\n",
    "\tglobal model\n",
    "\treturn construct_word_embedding(model, torch.tensor([tokenizer.pad_token_id], device=device))\n",
    "\n",
    "def get_tokens(text_ids):\n",
    "\tglobal tokenizer\n",
    "\treturn tokenizer.convert_ids_to_tokens(text_ids.squeeze())\n",
    "\n",
    "def get_inputs(text, device):\n",
    "\tglobal model, tokenizer\n",
    "\tref_token_id = tokenizer.mask_token_id\n",
    "\tsep_token_id = tokenizer.sep_token_id\n",
    "\tcls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\tinput_ids, ref_input_ids\t\t= construct_input_ref_pair(tokenizer, text, ref_token_id, sep_token_id, cls_token_id, device)\n",
    "\tposition_ids, ref_position_ids\t= construct_input_ref_pos_id_pair(input_ids, device)\n",
    "# \ttype_ids, ref_type_ids\t\t\t= construct_input_ref_token_type_pair(input_ids, device)\n",
    "\tattention_mask\t\t\t\t\t= construct_attention_mask(input_ids)\n",
    "\n",
    "\t(input_embed, ref_input_embed), (position_embed, ref_position_embed) = \\\n",
    "\t\t\t\tconstruct_sub_embedding(model, input_ids, ref_input_ids, position_ids, ref_position_ids)\n",
    "\n",
    "\treturn [input_ids, ref_input_ids, input_embed, ref_input_embed, position_embed, ref_position_embed, None, None, attention_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a11a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features\t\t= get_word_embeddings().cpu().detach().numpy()\n",
    "word_idx_map\t\t= tokenizer.vocab\n",
    "A\t\t\t\t\t= kneighbors_graph(word_features, 500, mode='distance', n_jobs=-1)\n",
    "\n",
    "# knn_fname = f\"knn/{'bert'}_{'sst2'}_{500}.pkl\"\n",
    "# with open(knn_fname, 'wb') as f:\n",
    "#     pickle.dump([word_idx_map, word_features, A], f)\n",
    "\n",
    "# print(f'Written KNN data at {knn_fname}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6184b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary_data = load_mappings('sst2', knn_nbrs=500)\n",
    "auxiliary_data = [word_idx_map, word_features, A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a055e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attribution function\n",
    "attr_func = DiscretetizedIntegratedGradients(nn_forward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "204614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= load_dataset('glue', 'sst2')['test']\n",
    "data= list(zip(dataset['sentence'], dataset['label'], dataset['idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51c644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "\n",
    "\n",
    "def calculate_attributions(inputs, device, attr_func, base_token_emb, nn_forward_func, get_tokens):\n",
    "    # computes the attributions for given input\n",
    "\n",
    "    # move inputs to main device\n",
    "    inp = [x.to(device) if x is not None else None for x in inputs]\n",
    "\n",
    "    # compute attribution\n",
    "    scaled_features, input_ids, ref_input_ids, input_embed, ref_input_embed, position_embed, ref_position_embed, type_embed, ref_type_embed, attention_mask = inp\n",
    "    attr, deltaa = run_dig_explanation(attr_func, scaled_features, position_embed, type_embed, attention_mask, 63)\n",
    "\n",
    "    # compute metrics\n",
    "    log_odd, pred\t= eval_log_odds(nn_forward_func, input_embed, position_embed, type_embed, attention_mask, base_token_emb, attr, topk=20)\n",
    "    comp\t\t\t= eval_comprehensiveness(nn_forward_func, input_embed, position_embed, type_embed, attention_mask, base_token_emb, attr, topk=20)\n",
    "    suff\t\t\t= eval_sufficiency(nn_forward_func, input_embed, position_embed, type_embed, attention_mask, base_token_emb, attr, topk=20)\n",
    "\n",
    "    #return log_odd\n",
    "    return log_odd, comp, suff, attr, deltaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3518ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting attribution computation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                      | 100/1821 [1:41:44<26:24:33, 55.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.5521 Comprehensiveness:  0.382 Sufficiency:  0.1784 Avg delta:  tensor([1.1244]) Avg delta pct: tensor([45.1509])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▏                                                                 | 200/1821 [3:06:19<23:16:18, 51.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.4505 Comprehensiveness:  0.3553 Sufficiency:  0.1674 Avg delta:  tensor([1.2044]) Avg delta pct: tensor([44.5725])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▏                                                             | 300/1821 [4:42:55<20:28:13, 48.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.5045 Comprehensiveness:  0.3559 Sufficiency:  0.153 Avg delta:  tensor([1.2980]) Avg delta pct: tensor([47.6614])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▎                                                         | 400/1821 [6:13:10<20:31:38, 52.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.5567 Comprehensiveness:  0.3499 Sufficiency:  0.1557 Avg delta:  tensor([1.4026]) Avg delta pct: tensor([62.9757])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████▎                                                     | 500/1821 [7:45:54<20:52:01, 56.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.5475 Comprehensiveness:  0.3573 Sufficiency:  0.1719 Avg delta:  tensor([1.4079]) Avg delta pct: tensor([64.5393])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████▍                                                 | 600/1821 [9:31:12<28:15:25, 83.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.5892 Comprehensiveness:  0.3602 Sufficiency:  0.1689 Avg delta:  tensor([1.4147]) Avg delta pct: tensor([62.7377])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████                                             | 700/1821 [11:17:46<19:14:49, 61.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6566 Comprehensiveness:  0.3762 Sufficiency:  0.1734 Avg delta:  tensor([1.3847]) Avg delta pct: tensor([59.4636])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████                                         | 800/1821 [12:58:35<20:21:18, 71.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6083 Comprehensiveness:  0.3747 Sufficiency:  0.1696 Avg delta:  tensor([1.3381]) Avg delta pct: tensor([55.8391])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████████████████████████████████████                                     | 900/1821 [15:05:57<20:10:50, 78.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6201 Comprehensiveness:  0.3783 Sufficiency:  0.1685 Avg delta:  tensor([1.3486]) Avg delta pct: tensor([56.2470])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████▌                                | 1000/1821 [16:58:37<17:35:57, 77.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6196 Comprehensiveness:  0.3787 Sufficiency:  0.1697 Avg delta:  tensor([1.3877]) Avg delta pct: tensor([56.5081])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████                             | 1100/1821 [18:59:30<8:10:21, 40.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6085 Comprehensiveness:  0.3746 Sufficiency:  0.173 Avg delta:  tensor([1.3783]) Avg delta pct: tensor([58.3925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████▍                        | 1200/1821 [20:45:03<11:08:26, 64.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6138 Comprehensiveness:  0.3761 Sufficiency:  0.1688 Avg delta:  tensor([1.3660]) Avg delta pct: tensor([57.1053])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████                     | 1300/1821 [22:36:18<9:27:17, 65.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6126 Comprehensiveness:  0.3821 Sufficiency:  0.1712 Avg delta:  tensor([1.3689]) Avg delta pct: tensor([57.6111])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|████████████████████████████████████████████████████████                 | 1400/1821 [24:18:43<4:44:47, 40.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6424 Comprehensiveness:  0.3868 Sufficiency:  0.1696 Avg delta:  tensor([1.3638]) Avg delta pct: tensor([56.5529])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████▏            | 1500/1821 [26:15:50<6:59:24, 78.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6561 Comprehensiveness:  0.3875 Sufficiency:  0.1672 Avg delta:  tensor([1.3588]) Avg delta pct: tensor([56.3230])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████▏        | 1600/1821 [28:15:05<3:45:59, 61.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6428 Comprehensiveness:  0.3856 Sufficiency:  0.1662 Avg delta:  tensor([1.3619]) Avg delta pct: tensor([56.2381])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████████████████████████████████▏    | 1700/1821 [30:14:11<2:37:58, 78.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6393 Comprehensiveness:  0.3883 Sufficiency:  0.1657 Avg delta:  tensor([1.3534]) Avg delta pct: tensor([56.4494])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████████████████████████████████████████▏| 1800/1821 [32:14:57<24:19, 69.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6415 Comprehensiveness:  0.3874 Sufficiency:  0.164 Avg delta:  tensor([1.3450]) Avg delta pct: tensor([55.4463])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1821/1821 [32:36:02<00:00, 64.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-odds:  -1.6529 Comprehensiveness:  0.3888 Sufficiency:  0.1647 Avg delta:  tensor([1.3473]) Avg delta pct: tensor([56.0178])\n",
      "CPU times: total: 1d 20h 59min 8s\n",
      "Wall time: 1d 8h 36min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get ref token embedding\n",
    "base_token_emb = get_base_token_emb(device)\n",
    "\n",
    "# compute the DIG attributions for all the inputs\n",
    "print('Starting attribution computation...')\n",
    "inputs,delta_pcs_list = [],[]\n",
    "log_odds, comps, suffs, deltas, delta_pcs, count = 0, 0, 0, 0, 0, 0\n",
    "print_step = 100\n",
    "for row in tqdm(data):\n",
    "    inp = get_inputs(row[0], device)\n",
    "    input_ids, ref_input_ids, input_embed, ref_input_embed, position_embed, ref_position_embed, type_embed, ref_type_embed, attention_mask = inp\n",
    "    scaled_features \t\t= monotonic_paths.scale_inputs(input_ids.squeeze().tolist(), ref_input_ids.squeeze().tolist(),\\\n",
    "                                        device, auxiliary_data, method =\"UIG\", steps=30, nbrs = 50, factor=1, strategy='maxcount')\n",
    "    inputs\t\t\t\t\t= [scaled_features, input_ids, ref_input_ids, input_embed, ref_input_embed, position_embed, ref_position_embed, type_embed, ref_type_embed, attention_mask]\n",
    "    log_odd, comp, suff, attrib, delta= calculate_attributions(inputs, device, attr_func, base_token_emb, nn_forward_func, get_tokens)\n",
    "    scaled_features_tpl = _format_input(scaled_features)\n",
    "    start_point, end_point = _format_input(scaled_features_tpl[0][0].unsqueeze(0)), _format_input(scaled_features_tpl[0][-1].unsqueeze(0))# baselines, inputs (only works for one input, i.e. len(tuple) == 1)\n",
    "    F_diff = (nn_forward_func(end_point[0],attention_mask,position_embed,type_embed).squeeze() - \\\n",
    "             nn_forward_func(start_point[0],attention_mask,position_embed,type_embed).squeeze()).detach().numpy()\n",
    "    delta_pc = delta/F_diff*100\n",
    "    log_odds\t+= log_odd\n",
    "    comps\t\t+= comp\n",
    "    suffs \t\t+= suff\n",
    "    deltas+= np.abs(delta)\n",
    "    delta_pcs+= np.abs(delta_pc)\n",
    "    delta_pcs_list.append(torch.abs(delta_pc).item())\n",
    "    count\t\t+= 1\n",
    "\n",
    "    # print the metrics\n",
    "    if count % print_step == 0:\n",
    "        print('Log-odds: ', np.round(log_odds / count, 4), 'Comprehensiveness: ', np.round(comps / count, 4), \n",
    "              'Sufficiency: ', np.round(suffs / count, 4),  'Avg delta: ', np.round(deltas / count, 4), \n",
    "              'Avg delta pct:', np.round(delta_pcs / count, 4))\n",
    "\n",
    "print('Log-odds: ', np.round(log_odds / count, 4), 'Comprehensiveness: ', np.round(comps / count, 4), \n",
    "      'Sufficiency: ', np.round(suffs / count, 4), 'Avg delta: ', np.round(deltas / count, 4), \n",
    "              'Avg delta pct:', np.round(delta_pcs / count, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4801ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Distil_UIG_f1_mask_dpc.pkl', 'wb') as file:\n",
    "    pickle.dump(delta_pcs_list,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a3950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.221582412719727"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(delta_pcs_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
